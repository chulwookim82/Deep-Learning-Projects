{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichelt Allocation Project\n",
    "Chulwoo Kim \n",
    "\n",
    "[Background]\n",
    "Topic models automatically infer the topics discussed in a collection of documents. These topics can be used to summarize and organize documents, or used for featurization and dimensionality reduction in later stages of the data analysis.\n",
    "\n",
    "LDA (Latent Dirichlet Allocation) is one of the most successful topic model libraries. Use LDA in this exercise to derive ‘topics’ from the dataset provided, the code should be written in Python.\n",
    "\n",
    "The dataset could be obtained from Yelp’s webportal by accessing this url:\n",
    "https://www.yelp.com/dataset_challenge\n",
    "\n",
    "Your code should include the following steps:\n",
    "- Prepping the data\n",
    "  * Tokenizing\n",
    "  * Stopping\n",
    "  * Stemming\n",
    "- Construct a Document-term Matrix\n",
    "- Apply the LDA Model\n",
    "- Examine the results\n",
    "\n",
    "Describe how accurate your model is and what it needs to be done to make it better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "The size of this data downloaded from Yelp.com is about 3.4GB. Therefore, I decide to use the database system because it is impossible to upload it in memory. The total number of lines for the data loaded into the mongoDB is 4169501 lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pymongo import MongoClient\n",
    "\n",
    "dataset_file = 'dataset/yelp_academic_dataset_review.json'\n",
    "reviews_collection = MongoClient(\"mongodb://localhost:27017/\")[\"Yelp_Reviews\"][\"Reviews\"]\n",
    "\n",
    "with open(dataset_file, encoding='utf8') as dataset:\n",
    "    next(dataset)\n",
    "    for line in dataset:\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "        except ValueError:\n",
    "            print('Value Error')\n",
    "        if data[\"type\"] == \"review\":\n",
    "            reviews_collection.insert({\n",
    "                \"reviewId\": data[\"review_id\"],\n",
    "                \"business\": data[\"business_id\"],\n",
    "                \"text\": data[\"text\"]\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "In this step, I performed to remove the punctuation as well as tokenizing, stopping, and stemming to clear dataset. Also, I use only 2000 reviews because it takes a very long time to process all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from pymongo import MongoClient\n",
    "\n",
    "NUMBER_OF_TEST_REVIEWS = 2000\n",
    "\n",
    "reviews_collection = MongoClient(\"mongodb://localhost:27017/\")[\"Yelp_Reviews\"][\"Reviews\"]\n",
    "reviews_cursor = reviews_collection.find()\n",
    "reviewsCount = reviews_cursor.count()\n",
    "reviews_cursor.batch_size(1000)\n",
    "\n",
    "stopword = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "ps = PorterStemmer()\n",
    "\n",
    "words = []\n",
    "for i, review in enumerate(reviews_cursor):    \n",
    "    sentences = nltk.sent_tokenize(review[\"text\"].lower())\n",
    "    if i == NUMBER_OF_TEST_REVIEWS:\n",
    "        break;\n",
    "    for sentence in sentences:\n",
    "        # Remove punctuation\n",
    "        sentence = ''.join(ch for ch in sentence if ch not in punctuation)\n",
    "        # Remove number\n",
    "        sentence = ''.join(ch for ch in sentence if ch not in \"0123456789\")\n",
    "        tokens = nltk.word_tokenize(sentence)    \n",
    "        # Stemming & Removing stop words\n",
    "        text = [ps.stem(word) for word in tokens if word not in stopword]\n",
    "    words.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Construction of a document-term matrix\n",
    "\n",
    "If the data preprocessing is finished, it only left the words as a string type. We have to number it to apply this to the LDA model. I use two function, Dictionary and doc2bow. The dictionary function transfers each words to a unique number. After dictionary function perform, I execute the doc2bow function which converts dictionary into a bag-of-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "\n",
    "dictionary = corpora.Dictionary(doc for doc in words)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Applying the LDA model\n",
    "\n",
    "Latent Dirichlet allocation (LDA) is a topic model that generates topics based on word frequency from a set of documents. LDA is particularly useful for finding reasonably accurate mixtures of topics within a given document set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(doc_term_matrix, num_topics=10, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Examining the results\n",
    "\n",
    "Using the print_topics function, we can get a specific output. This result shows us topics and their probability. You can guess the sentence;I would definitely come back here, by looking at the topic of the first review, but it is hard to see it as a meaningful topic. Next topics such as best drink make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.076*\"back\" + 0.031*\"go\" + 0.030*\"come\" + 0.020*\"definit\" + 0.015*\"would\"'), (1, '0.015*\"place\" + 0.013*\"one\" + 0.010*\"best\" + 0.009*\"drink\" + 0.009*\"go\"'), (2, '0.021*\"good\" + 0.015*\"servic\" + 0.012*\"time\" + 0.010*\"im\" + 0.009*\"go\"'), (3, '0.015*\"time\" + 0.013*\"love\" + 0.012*\"like\" + 0.010*\"friendli\" + 0.009*\"staff\"'), (4, '0.021*\"place\" + 0.015*\"good\" + 0.013*\"get\" + 0.010*\"lot\" + 0.009*\"know\"'), (5, '0.013*\"food\" + 0.011*\"die\" + 0.011*\"money\" + 0.010*\"und\" + 0.008*\"happi\"'), (6, '0.016*\"even\" + 0.014*\"place\" + 0.013*\"price\" + 0.011*\"get\" + 0.009*\"would\"'), (7, '0.060*\"recommend\" + 0.023*\"highli\" + 0.021*\"would\" + 0.012*\"go\" + 0.012*\"realli\"'), (8, '0.023*\"food\" + 0.016*\"good\" + 0.012*\"great\" + 0.012*\"enjoy\" + 0.011*\"place\"'), (9, '0.021*\"go\" + 0.020*\"great\" + 0.019*\"definit\" + 0.014*\"would\" + 0.014*\"place\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=10, num_words=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "If I set up the properties like 10 topics and 5 words, the topics are mostly significant but it is not perfect. I would say this model fairly make result, even if I cannot calculate specific accuracy value because this is not a classification problem.\n",
    "\n",
    "Also, we can think about the way to make better result.\n",
    "Here are some way to improve performence:\n",
    "1. It has to learn the LDA model having more data. I just use 2,000 reviews but the more data, the better results. Instead, the learning time will increase tremendously.\n",
    "2. Well-tuned properties of the lda model, such as passes and num_topics, can improve performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
